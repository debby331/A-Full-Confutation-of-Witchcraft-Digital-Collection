# A-Full-Confutation-of-Witchcraft-Digital-Collection
Digital Humanities Project

**Author:** Henry Charles Lea Library (University of Pennsylvania) 

**Title:** A full confutation of witchcraft: more particularly of the depositions against Jane Wenham, lately condemned for a witch at Hertford : in which modern notions of witches are overthrown, and the ill consequences of such doctrines are exposed by arguments,

**Date of publication:** 1712 (early works to 1800)

**Provenance:** Lea, Henry Charles, 1825-1909 (autograph) (bookplate); Cox, Charles F. (bookplate)

**Publisher:** Printed for J. Baker .. London, England

**Colenda link:** https://colenda.library.upenn.edu/catalog/81431-p3wd3r356

**Catalogue:** https://find.library.upenn.edu/catalog/9944284413503681?hld_id=resource_link_0 

**Physical Location:** University of Pennsylvania, Kislak Center for Special Collections, Rare Books and Manuscripts, S-22.2.6

**Collection:** Henry Charles Lea Library (University of Pennsylvania)

**Catalog ID/Bibnumber:** 9944284413503681

**Page count:** 38

**Attribution:** Provided by the University of Pennsylvania Libraries.

        Metadata is a critical tool to digital humanities studies to enable greater classification of works as they 
        are transferred to online platforms. For my work, “A full confutation of witchcraft”, it was difficult to pick 
        specific metadata that could differentiate the text. The style of each pages was nearly identical: black text, 
        yellowish pages, and similar printing styles. I needed to start with important and broad metadata such as the 
        file name (for clarification) and its corresponding page number in the text as the book’s digital copies were 
        in interesting orders with some pages skipped or even repeated. I also felt the need to note details of the page 
        that are lost in a digital copy such as dimensions (of height and width) as well as color (in the event the scanner 
        could only scan black and white images) of both the text and the page. I also wanted a way to differentiate the near 
        identical pages from a visual sense without having to read every word. I struggled to find categories to encompass 
        these visual differences on such similar looking pages but ultimately chose to use the presence/absence of italics 
        and additionally any visual imperfections (broken into the categories of scratches, smudges, dots and low ink 
        appearances). Ultimately, the metadata enables the documents to be viewed from a more descriptive perspective 
        and categorized among the twenty five page collection.

        The OCR reader is a very helpful tool for further analyzing digital sources. I started with Adobe Acrobat OCR, and it was very 
inaccurate, missing several letters and punctuation, and even failing to identify the ſ symbol for a long s. I pivoted to using ChatGPT to see 
if it had better OCR, and it was exceptional. It was highly accurate, able to switch all “ſ” characters to “s” manually, and to distinguish 
between many characters despite scratches, smudges, and varying ink levels (a big relief and change from the very inaccurate Adobe). Some very 
infrequent and minor errors occurred, such as when words like Persuasion were spelled Perſwaſion, with the OCR scanner not fully able to 
differentiate them. Additionally, when the entire PDF was placed for OCR scanning, the pages were severely misaligned, with sections being 
entirely off. This required sections to be entirely rewritten or entail searching for where the transcribed section could fit. Once I switched 
to manually prompting for each page with the original image, it worked with near-perfect accuracy. Originally, the AI combined words that had 
been separated by lines, for example, if improbabilities was spelled improbabi- on the first line and lities on the second, the scanner would 
combine and display the whole word “improbabilities”. However, I was able to prompt it to keep the original line spacing, including broken-up 
words, and it executed that command exceptionally well (especially in comparison to Adobe’s attempt). With the use of the added prompts to 
maintain line spacing and the single-image processing, very few errors occurred, resulting in fewer corrections (after hours of trial and error 
to get to this stage). I luckily did not have tables, images, or any difficult spacing that would be hard for the OCR to encounter. Overall, 
the process took several hours to set up and establish a routine, many pages and tests later. The careful review of the document and the 
modification of commands to preserve every line break, punctuation mark, capitalization, and hyphenation exactly as they appear in the 
original, and to change all long “ſ” to modern “s”, ultimately allowed the OCR reading to be completed. 

        
